{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cefdaa0-2ed5-4eb9-a924-ff4395b89a3e",
   "metadata": {},
   "source": [
    "# EX0. Hello World\n",
    "\n",
    "Welcome to the **Hello World** example for the BRAD Python package! This tutorial is designed to help you get started with BRAD by demonstrating its core functionality. You'll learn how to:\n",
    "\n",
    "1. Import the BRAD package.\n",
    "2. Create and configure a BRAD `Agent` instance.\n",
    "3. Select and configure different large language models (LLMs).\n",
    "4. Integrate BRAD with other LangChain workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18b480-0d1a-4667-96bf-4c5157f326e3",
   "metadata": {},
   "source": [
    "## 1. Import Module\n",
    "\n",
    "To begin, ensure you have installed the BRAD package. The package can be installed either directly from the project [Github](https://github.com/Jpickard1/BRAD/) or from the [Python Package Index (i.e. PyPI)](https://pypi.org/project/BRAD-Chat/) using `pip`:\n",
    "\n",
    "```\n",
    "!pip install -U BRAD-Chat\n",
    "```\n",
    "\n",
    "See the software manual distribution section [here](https://brad-bioinformatics-retrieval-augmented-data.readthedocs.io/en/latest/distribution.html) for additional ways to install the package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2ef84-e63b-45d0-9cfe-2596490c81c0",
   "metadata": {},
   "source": [
    "### Quickstart  \n",
    "\n",
    "Getting started with BRAD is simple and efficient. Once the package is installed, follow these steps for the quickest way to begin:\n",
    "\n",
    "1. **Import the Chat Module**  \n",
    "   Import the chat module with:  \n",
    "   `from BRAD import chat`\n",
    "\n",
    "2. **Start Chat**  \n",
    "   To start the chat, simply call the `chat()` method. By default, this will prompt you to:\n",
    "\n",
    "   - Enter your OpenAI API key\n",
    "   - Select a Retrieval-Augmented Generation (RAG) database (you can enter `N` to skip this step)\n",
    "\n",
    "3. **Welcome Message**  \n",
    "   Once you've entered the required information, a welcome message will be displayed. This message will:\n",
    "\n",
    "   - Provide the location of the output directory, where information, data, or downloads from the chat will be stored.\n",
    "   - Prompt you to enter an input and begin chatting with the model.\n",
    "\n",
    "In later sections, we will show how to change these configurations to customize your BRAD experience.\n",
    "\n",
    "Below, the code was run to perform this process. The user asks the chat session who it is and what it can do, and then closes the conversation with the keyword `exit`.  \n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4a9f89-6333-41d6-9aa9-a50e88fa7cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Open AI API key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Would you like to use a database with BRAD [Y/N]?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-21 14:29:47 INFO semantic_router.utils.logger local\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to RAG! The chat log from this conversation will be saved to /home/jpic/C:\\Users\\jpic\\Documents\\BRAD/November 21, 2024 at 02:29:34 PM/log.json. How can I help?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input >>  who are you and what can you do?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "BRAD >> 1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 14:35:30,384 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am BRAD, a Bioinformatic Retrieval Augmented Data chatbot. I specialize in biology, bioinformatics, genetics, and data science. I can provide information, answer questions, search databases, run pipelines, analyze codes, and more to assist with various biological and data-related inquiries. Feel free to ask me anything related to my expertise!\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input >>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for chatting today! I hope to talk soon, and don't forget that a record of this conversation is available at: /home/jpic/C:\\Users\\jpic\\Documents\\BRAD/November 21, 2024 at 02:29:34 PM/log.json\n"
     ]
    }
   ],
   "source": [
    "from BRAD import chat\n",
    "chat.chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd232119-8dac-4294-9495-b8c3a571873c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Similar to importing the `chat` module in this Jupyter Notebook, the codes can be run similarly from the command line interface. Navigating in the command line to the root of the main project's repository, the following command will open a similar chat session:\n",
    "\n",
    "`python BRAD/chat.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb32de8-5920-48c4-85c0-c6c25e81cfad",
   "metadata": {},
   "source": [
    "### `Agent` Module  \n",
    "\n",
    "When using the `chat` module above, the `Agent` model is used under the hood to create an instance of an LLM-powered agent, which has access to different tools. Specifically, the `chat` module is a wrapper that executes the command: `Agent().chat()`\n",
    "\n",
    "The `Agent` class is the primary way to configure BRAD. It allows you to set different options such as:\n",
    "\n",
    "- Selecting a different `LLM` (Large Language Model)\n",
    "- Choosing the correct RAG database\n",
    "- Restarting previous chat sessions\n",
    "- And more...\n",
    "\n",
    "In the following sections, we will introduce important configurations as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273f628c-1380-4cb7-9a5d-3581c5bb0245",
   "metadata": {},
   "source": [
    "## 2. Create an `Agent`\n",
    "\n",
    "This section will show how to:\n",
    "1. create an `Agent`,\n",
    "2. interface with the `Agent`,\n",
    "3. and set different configurations.\n",
    "\n",
    "To import the `Agent` class, we use the command:\n",
    "\n",
    "`from BRAD import agent`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03726d7-d077-4108-be47-2c2ca2ffd2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BRAD import agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9bba85-10e6-4467-81f1-191fa2dca48d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "---\n",
    "\n",
    "### `Agent` Constructor\n",
    "\n",
    "After importing the `Agent` class, we can use the following constructor to create an `Agent`. This will have similar prompts to the user as given by the `chat` module.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4bfe46-c71c-47dc-a0e1-fc521ad82b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Open AI API key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Would you like to use a database with BRAD [Y/N]?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 15:43:15 INFO semantic_router.utils.logger local\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to RAG! The chat log from this conversation will be saved to /home/jpic/C:\\Users\\jpic\\Documents\\BRAD/November 20, 2024 at 03:42:41 PM/log.json. How can I help?\n"
     ]
    }
   ],
   "source": [
    "bot = agent.Agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998af5d-cbf0-461d-bc39-e684648c4751",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here again, an API key is requested. By default, the `Agent` class will use LLMs from OpenAI. When the user is prompted to enter an API key, these are saved within:\n",
    "\n",
    "`os.environ[\"OPENAI_API_KEY\"]`\n",
    "\n",
    "Alternatively, when the user selects models from NVIDIA, as we will do in the following sections, the API keys are stored in:\n",
    "\n",
    "`os.environ[\"NVIDIA_API_KEY\"]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f3534e-d75b-45d3-961b-ebb0032c35c4",
   "metadata": {},
   "source": [
    "### Chat with the `Agent`\n",
    "\n",
    "Once the `Agent` has been created, there are two methods to interact with it:\n",
    "\n",
    "- `chat()`: this method opens an interactive chat session, similar to the one produced by the `chat` module.\n",
    "- `invoke()`: this method responds to a single user query.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bb7642c-9429-43fa-a498-5b4ef8dd2303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "BRAD >> 2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 15:07:35,339 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am BRAD, a Bioinformatic Retrieval Augmented Data chatbot specializing in biology, bioinformatics, genetics, and data science. I can provide information on various topics within these fields, search for relevant articles, analyze data using pipelines and codes, and access databases like Gene Ontology and Enrichr. Feel free to ask me any questions you have related to biology and data science!\n"
     ]
    }
   ],
   "source": [
    "response = bot.invoke(\"Who are you and what can you do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd77d409-328f-4beb-a8d7-f0aeac358400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input >>  what types of problems can you solve?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "BRAD >> 3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 15:08:03,452 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRAD: I can help solve a wide range of problems related to biology, bioinformatics, genetics, and data science. This includes analyzing genetic data, identifying patterns in biological datasets, predicting protein structures, exploring gene functions, and much more. Whether you need assistance with data analysis, literature search, pipeline creation, or code optimization, I am here to help with various problems in these fields. Feel free to ask me any specific questions you have!\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input >>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for chatting today! I hope to talk soon, and don't forget that a record of this conversation is available at: /home/jpic/C:\\Users\\jpic\\Documents\\BRAD/November 21, 2024 at 03:07:16 PM/log.json\n"
     ]
    }
   ],
   "source": [
    "bot.chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d59b6-038d-447e-bd01-ab6e5a5eed98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Agent State\n",
    "\n",
    "The purpose of the `Agent` class is to maintain the state or memory of the system. For example, the `Agent` can query the LLM, but in order to provide the LLM with access to previous parts of the conversation, the `Agent` tracks the conversation history, so that it can be resupplied to the LLM. Additional information to track the configurations, input, output, process of using different tool modules and more are also tracked by the state, which has the following shema:\n",
    "\n",
    "```\n",
    ">>> Agent.state = {\n",
    ">>> 'config'            : {\n",
    ">>>     <configuration variables>\n",
    ">>> },\n",
    ">>> 'prompt'            : <user input>,\n",
    ">>> 'output'            : <streaming output of Agent>,\n",
    ">>> 'memory'            : <agent memory>,\n",
    ">>> 'process'           : {\n",
    ">>>     'MODULE'        : <Tool module used to respond to user input>,\n",
    ">>>     <module specific information>: {\n",
    ">>>         ...\n",
    ">>>     }\n",
    ">>> },\n",
    ">>> 'llm-api-calls'     : <number of LLM calls used by Agent>,\n",
    ">>> ...\n",
    ">>> }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63848752-2e5f-4cf4-a57c-835bfcbc08ec",
   "metadata": {},
   "source": [
    "In addition to maintaining a state in memory, a unique output directory is assigned to each `Agent`. These directories are where all information, data, or downloads related to the `Agent` are stored. These output directories allow information or memory to persist even when the `Agent` is turned off or deleted, allowing it to be restarted from the same position. Within the output directory, the `log.json` file is used to record each step or action taken by the `Agent` including every query of the LLM. The log file follows the following schema:s\n",
    "3. debuging vs. nondebuggin mode\n",
    "4. and more\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f49286-6d94-44a3-b183-f6a26f84754c",
   "metadata": {},
   "source": [
    "```\n",
    ">>> [\n",
    ">>>     0: {\n",
    ">>>         'TIME'   : <time stamp>,\n",
    ">>>         'PROMPT' : <input from the user or preplanned prompt>,\n",
    ">>>         'OUTPUT' : <output displayed to the user>,\n",
    ">>>         'STATUS' : {\n",
    ">>>                 'LLM' : <primary large language model being used>,\n",
    ">>>                 'Databases' : {\n",
    ">>>                         'RAG' : <primary data base>,\n",
    ">>>                         <other databases>: <more databases can be added>,\n",
    ">>>                         ...\n",
    ">>>                     },\n",
    ">>>                 'config' : {\n",
    ">>>                         'debug' : True,\n",
    ">>>                         'output-directory' : <path to output directory>,\n",
    ">>>                         ...\n",
    ">>>                     }\n",
    ">>>             },\n",
    ">>>         'PROCESS' : {\n",
    ">>>                 'MODULE' : <name of module i.e. RAG, DATABASE, CODER, etc.>\n",
    ">>>                 'STEPS' [\n",
    ">>>                         # information for a particular step involved in executing the module. Some examples\n",
    ">>>                         {\n",
    ">>>                            'LLM' : <language model>,\n",
    ">>>                            'prompt template' : <prompt template>,\n",
    ">>>                            'model input' : <input to model>,\n",
    ">>>                            'model output' : <output of model>\n",
    ">>>                         },\n",
    ">>>                         {\n",
    ">>>                            'func' : 'rag.retreival',\n",
    ">>>                            'num articles retrieved' : 10,\n",
    ">>>                            'multiquery' : True,\n",
    ">>>                            'compression' : True,\n",
    ">>>                         }\n",
    ">>>                     ]\n",
    ">>>             },\n",
    ">>>         'PLANNED' : [\n",
    ">>>                 <Next prompt in queue>,\n",
    ">>>                 ...\n",
    ">>>             ]\n",
    ">>>     },\n",
    ">>> 1 : {\n",
    ">>>         'TIME'   : <time stamp>,\n",
    ">>>         'PROMPT' : <second prompt>,\n",
    ">>>         ...\n",
    ">>>     },\n",
    ">>> ...\n",
    ">>> ]:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410db835-41df-4bf9-9b15-8c80003f401f",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "\n",
    "To configure an `Agent` for different tasks, the `Agents` can be supplied `.json` files during construction to specify information such as:\n",
    "1. where to place the output directory\n",
    "2. how to use different tools\n",
    "3. debuging vs. nondebuggin mode\n",
    "4. and more\n",
    "\n",
    "A default configuration file is build into the python package and is available [here](https://github.com/Jpickard1/BRAD/blob/main/BRAD/config/config.json). The default values can be overwritten by providing similarly structured files as input when constructing an `Agent`. We will demonstrate this in the subsequent example notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaefe91-2791-4f9a-95ee-7ae6f169fc54",
   "metadata": {},
   "source": [
    "## 3. LLM Selection\n",
    "\n",
    "The BRAD package is interoperable for different LLMs. Any [LangChain](https://www.langchain.com/) compatible LLM can be used for the argument `Agent(llm=<here>)` to construct the `Agent`. BRAD contains code to load LLMs using:\n",
    "1. [OpenAI's API](https://openai.com/)\n",
    "2. [NVIDIA's API](https://build.nvidia.com/explore/discover)\n",
    "3. [LlamaCpp](https://github.com/ggerganov/llama.cpp)\n",
    "\n",
    "When using the APIs, it is the responsibility of the user to obtain API keys for the respective services, and when using a locally run LLM with LlamaCpp, the user must provide a path to the `.guff` file and ensure appropriate hardware to run inference on the LLM. To load LLMs from each of these three locations, the following module and methods can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1140e0-ff1d-4464-bef7-26c67bb67398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BRAD import llms\n",
    "\n",
    "model = llms.load_openai(\n",
    "    model_name='gpt-3.5-turbo-0125',\n",
    "    api_key='XXXXX'\n",
    "    temperature='1.0'\n",
    ")\n",
    "\n",
    "model = llms.load_nvidia(\n",
    "    model_name='meta/llama3-70b-instruct',\n",
    "    api_key='XXXXX'\n",
    "    temperature='1.0'\n",
    ")\n",
    "\n",
    "model = llmsload_llama(model_path = <path to .guff file>,\n",
    "               n_ctx = 4096,\n",
    "               max_tokens = 1000,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f51499-290d-44f7-b96e-b84bcdd8ea62",
   "metadata": {},
   "source": [
    "These models can then be input as the LLM of an `Agent` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47672582-797f-47a9-a7a6-9a5131b2bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = agent.Agent(\n",
    "    llm = model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3791b68f-3379-4307-ae1b-c99dede0d27c",
   "metadata": {},
   "source": [
    "## 4. Integration to LangChain\n",
    "\n",
    "BRAD is built atop the [LangChain](https://www.langchain.com/) environment, which is a comprehensive framework for developing LLM powered applications. Within each method or tool of BRAD is several LangChain chains that allow the LLM to make decisions regarding how to use the tools.\n",
    "\n",
    "To make BRAD consistent with LangChain, an `Agent` can be reintegrated into a LangChain tool. To do the, the `Agent.to_langchain` method provides an interface to port an `Agent` to be compatible with `langchain_core.language_models.llms`. This works as follows:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a453bc3-044e-42ce-a91b-e578c2925156",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmBot = bot.to_langchain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ae928-de10-4c00-af9a-45a9d61bef17",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Then `llmBot` can be placed into any LangChain environment, while retaining full access to BRAD's functionality.\n",
    "\n",
    "Thank you for exploring the Hello World notebook! Please proceed to the next notebooks to dive deeper.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
