{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cefdaa0-2ed5-4eb9-a924-ff4395b89a3e",
   "metadata": {},
   "source": [
    "# Hello World\n",
    "\n",
    "This example introduces the BRAD python package including:\n",
    "1. how to import the module\n",
    "2. how to create a BRAD `Agent` instance\n",
    "3. how to select and configure different LLMs\n",
    "4. how to integrate BRAD into other LangChain codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18b480-0d1a-4667-96bf-4c5157f326e3",
   "metadata": {},
   "source": [
    "## 1. Import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a948541-21ea-47c1-b9cf-2dc56759e810",
   "metadata": {},
   "source": [
    "### Quickstart: Just Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be4a9f89-6333-41d6-9aa9-a50e88fa7cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Would you like to use a database with BRAD [Y/N]?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 16:00:58 INFO semantic_router.utils.logger local\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to RAG! The chat log from this conversation will be saved to /home/jpic/C:\\Users\\jpic\\Documents\\BRAD/November 20, 2024 at 04:00:55 PM/log.json. How can I help?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input >>  who are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "BRAD >> 1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 16:01:07,019 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am BRAD, a Bioinformatic Retrieval Augmented Data chatbot specializing in biology, bioinformatics, genetics, and data science. I can provide information and assistance on a wide range of topics within these fields using a combination of text database retrieval, web searching, bioinformatics database querying, pipeline running, and code analysis. How can I assist you today?\n",
      "WRITE LOG\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input >>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for chatting today! I hope to talk soon, and don't forget that a record of this conversation is available at: /home/jpic/C:\\Users\\jpic\\Documents\\BRAD/November 20, 2024 at 04:00:55 PM/log.json\n"
     ]
    }
   ],
   "source": [
    "from BRAD import chat\n",
    "chat.chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8ac5a-8984-4673-9e33-af24144daed4",
   "metadata": {},
   "source": [
    "### Agent Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03bba5a-ab9b-4593-b83a-5118fa55e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BRAD import agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273f628c-1380-4cb7-9a5d-3581c5bb0245",
   "metadata": {},
   "source": [
    "## 2. Create an `Agent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226caca5-7cb9-42dd-9108-1e3ce31da4b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4bfe46-c71c-47dc-a0e1-fc521ad82b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Open AI API key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Would you like to use a database with BRAD [Y/N]?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 15:43:15 INFO semantic_router.utils.logger local\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to RAG! The chat log from this conversation will be saved to /home/jpic/C:\\Users\\jpic\\Documents\\BRAD/November 20, 2024 at 03:42:41 PM/log.json. How can I help?\n"
     ]
    }
   ],
   "source": [
    "bot = agent.Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb7642c-9429-43fa-a498-5b4ef8dd2303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "BRAD >> 1: \n"
     ]
    }
   ],
   "source": [
    "bot.invoke(\"Who are you and what can you do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d80950be-43aa-4656-8514-e2b282dd6393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Agent in module BRAD.agent:\n",
      "\n",
      "class Agent(builtins.object)\n",
      " |  Agent(model_path='/nfs/turbo/umms-indikar/shared/projects/RAG/models/llama-2-7b-chat.Q8_0.gguf', persist_directory='/nfs/turbo/umms-indikar/shared/projects/RAG/databases/DigitalLibrary-10-June-2024/', llm=None, ragvectordb=None, embeddings_model=None, restart=None, start_path=None, tools=None, name='BRAD', max_api_calls=None, interactive=True, config=None, gui=False)\n",
      " |  \n",
      " |  This class organizes the agentic capabilities of BRAD. It facilitates interactions \n",
      " |  with external LLMs, tools, core modules, literature, and other databases while\n",
      " |  managing the chat state and history.\n",
      " |  \n",
      " |  Key functions include:\n",
      " |  \n",
      " |  1. **invoke(user_input)**: Responds to a single user input.\n",
      " |  2. **chat()**: Initiates an interactive session between the user and the BRAD agent.\n",
      " |  \n",
      " |  To address user queries, the agent employs semantic routing to select the appropriate tool module, generates responses using code from the chosen module, and tracks its state throughout the interaction.\n",
      " |  \n",
      " |  :param model_path: The path to the Llama model file, defaults to '/nfs/turbo/umms-indikar/shared/projects/RAG/models/llama-2-7b-chat.Q8_0.gguf'.\n",
      " |  :type model_path: str, optional\n",
      " |  :param persist_directory: The directory where the literature database is stored, defaults to \"/nfs/turbo/umms-indikar/shared/projects/RAG/databases/Transcription-Factors-5-10-2024/\".\n",
      " |  :type persist_directory: str, optional\n",
      " |  :param llm: The language model to be used. If None, it will be loaded within the function.\n",
      " |  :type llm: PreTrainedModel, optional\n",
      " |  :param ragvectordb: The RAG vector database to be used. If None, it will prompt the user to load it.\n",
      " |  :type ragvectordb: Chroma, optional\n",
      " |  :param embeddings_model: The embeddings model to be used. If None, it will be loaded within the function.\n",
      " |  :type embeddings_model: HuggingFaceEmbeddings, optional\n",
      " |  :param max_api_calls: The maximum number of api / llm calls BRAD can make\n",
      " |  :type max_api_calls: int, optional\n",
      " |  :param tools: The set of available tool modules. If None, all modules are available for use\n",
      " |  :type tools: list, optional\n",
      " |  :param gui: Indicates if the Agent is used in the GUI\n",
      " |  :type gui: boolean, optional\n",
      " |  \n",
      " |  :raises FileNotFoundError: If the specified model or database directories do not exist.\n",
      " |  :raises json.JSONDecodeError: If the configuration file contains invalid JSON.\n",
      " |  :raises KeyError: If required keys are missing from the configuration or chat status.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model_path='/nfs/turbo/umms-indikar/shared/projects/RAG/models/llama-2-7b-chat.Q8_0.gguf', persist_directory='/nfs/turbo/umms-indikar/shared/projects/RAG/databases/DigitalLibrary-10-June-2024/', llm=None, ragvectordb=None, embeddings_model=None, restart=None, start_path=None, tools=None, name='BRAD', max_api_calls=None, interactive=True, config=None, gui=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  chat(self)\n",
      " |      Opens an interactive chat session where users can execute a series of prompts.\n",
      " |      \n",
      " |      This method allows users to engage in a back-and-forth dialogue with the chatbot. \n",
      " |      Users can input queries, which the chatbot processes and responds to until \n",
      " |      the session is terminated. It supports both direct user input and queued prompts.\n",
      " |      \n",
      " |      The chat session maintains a record of the conversation and tracks the number \n",
      " |      of API calls made to language models. It ensures that the session can be exited \n",
      " |      gracefully and provides feedback about the conversation's context.\n",
      " |      \n",
      " |      :example:\n",
      " |          >>> agent.chat()\n",
      " |      \n",
      " |      :note:\n",
      " |          The session continues until the user explicitly decides to exit by \n",
      " |          inputting commands like \"exit\", \"quit\", or \"bye\".\n",
      " |      \n",
      " |          If a queue of prompts is available, the chatbot will process \n",
      " |          them in sequence rather than waiting for user input.\n",
      " |      \n",
      " |          The memory can be temporarily integrated to enrich queries, \n",
      " |          but its management is handled with care to avoid unintended \n",
      " |          modifications.\n",
      " |  \n",
      " |  chatbotHelp(self)\n",
      " |      Displays a help message to the user with information about the BRAD agents's capabilities and special commands.\n",
      " |  \n",
      " |  getLLMcalls(self, steps)\n",
      " |      Counts the number of times the LLM has been called used the agent's execution.\n",
      " |      \n",
      " |      :param steps: A list of steps from the agents log that have been executed\n",
      " |      :type steps: list\n",
      " |      \n",
      " |      :return: The total number of LLM calls made by the agent.\n",
      " |      :rtype: int\n",
      " |      \n",
      " |      :example:\n",
      " |          >>> num_calls = agent.getLLMcalls(steps)\n",
      " |  \n",
      " |  getModules(self)\n",
      " |      Returns a dictionary mapping module names to their corresponding function handles for various tasks.\n",
      " |      \n",
      " |      :param None: This function does not take any parameters.\n",
      " |      \n",
      " |      :raises None: This function does not raise any specific errors.\n",
      " |      \n",
      " |      :return: A dictionary where the keys are module names and the values are function handles for tasks such as querying Enrichr, web scraping, generating seaborn plots, querying documents, and calling Snakemake.\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  get_display(self)\n",
      " |      This function returns the history of all inputs/outputs to the agent. This is intended\n",
      " |      for use by the GUI, as it will allow the user to jump between sessions while loading in\n",
      " |      the history of the old session.\n",
      " |      \n",
      " |      :returns: a list of strings\n",
      " |      :rtype: list\n",
      " |  \n",
      " |  invoke(self, query)\n",
      " |      Executes a single query using the chatbot, similar to invoking a language model.\n",
      " |      \n",
      " |      This method processes the user input, determines the appropriate routing \n",
      " |      based on explicit commands or the content of the query, and generates a \n",
      " |      response using the selected module. It also manages the state of the \n",
      " |      chatbot throughout the execution.\n",
      " |      \n",
      " |      :param query: The user input to be processed by the chatbot.\n",
      " |      :type query: str\n",
      " |      \n",
      " |      :return: The output generated by the chatbot in response to the input query.\n",
      " |      :rtype: str\n",
      " |      \n",
      " |      :raises Exception: If an error occurs during the execution of the selected module.\n",
      " |      \n",
      " |      :example:\n",
      " |          >>> response = agent.invoke(\"What's the weather today?\")\n",
      " |      \n",
      " |      :note: \n",
      " |          Special commands recognized include:\n",
      " |          - \"exit\", \"quit\", \"q\", \"bye\": Ends the session.\n",
      " |          - \"help\": Displays help information.\n",
      " |          - \"/set\": Configures settings.\n",
      " |          - \"/force\": Forces the use of a specified routing function.\n",
      " |      \n",
      " |      This method logs the process and clears memory based on configuration settings.\n",
      " |  \n",
      " |  load_config(self, configfile=None)\n",
      " |      Loads the `Agent` configuration settings from a JSON file.\n",
      " |      \n",
      " |      :param None: This function does not take any parameters.\n",
      " |      \n",
      " |      :raises FileNotFoundError: If the configuration file is not found.\n",
      " |      :raises json.JSONDecodeError: If the configuration file contains invalid JSON.\n",
      " |      \n",
      " |      :return: A dictionary containing the configuration settings.\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  load_literature_db(self, persist_directory='/home/acicalo/BRAD/data/RAG_Database', db_name='DB_cosine_cSize_700_cOver_200')\n",
      " |      Loads a literature database using specified embedding model and settings.\n",
      " |      \n",
      " |      :param persist_directory: The directory where the database is stored, defaults to \"/nfs/turbo/umms-indikar/shared/projects/RAG/databases/Transcription-Factors-5-10-2024/\"\n",
      " |      :type persist_directory: str, optional\n",
      " |      \n",
      " |      :raises FileNotFoundError: If the specified directory does not exist or is inaccessible.\n",
      " |      :raises Warning: If the loaded database contains no articles.\n",
      " |      \n",
      " |      :return: A tuple containing the vector database and the embeddings model.\n",
      " |      :rtype: tuple\n",
      " |      \n",
      " |      The `persist_directory` should point to a directory that has this structure:\n",
      " |      \n",
      " |      >>> [Oct 16 19:28]  persist_directory\n",
      " |      ... └── [Oct 16 19:28]  DB_cosine_cSize_700_cOver_200\n",
      " |      ...     ├── [Oct 16 19:28]  aaa2c989-0e39-4be8-82b4-139ae2784c00\n",
      " |      ...     │   ├── [Oct 16 19:28]  data_level0.bin\n",
      " |      ...     │   ├── [Oct 16 19:28]  header.bin\n",
      " |      ...     │   ├── [Oct 16 19:28]  length.bin\n",
      " |      ...     │   └── [Oct 16 19:28]  link_lists.bin\n",
      " |      >>>     └── [Oct 16 19:28]  chroma.sqlite3\n",
      " |  \n",
      " |  load_state(self, config=None)\n",
      " |      Initializes and loads the agent state with default values and configuration settings.\n",
      " |      \n",
      " |      :param None: This function does not take any parameters.\n",
      " |      \n",
      " |      :raises FileNotFoundError: If the configuration file is not found.\n",
      " |      :raises json.JSONDecodeError: If the configuration file contains invalid JSON.\n",
      " |      \n",
      " |      :return: A dictionary representing the chat status with initial values and loaded configuration.\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  reconfig(self)\n",
      " |      Updates a specific configuration setting based on the given chat status and saves the updated configuration.\n",
      " |      \n",
      " |      :param chat_status: A dictionary containing the current chat status, including the prompt and configuration.\n",
      " |      :type chat_status: dict\n",
      " |      \n",
      " |      :raises KeyError: If the specified configuration key is not found in the chat status.\n",
      " |      :raises ValueError: If the value cannot be converted to an integer or float when applicable.\n",
      " |      \n",
      " |      :return: The updated chat status dictionary.\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  resetMemory(self)\n",
      " |      Resets the state of the memory by restoring the main memory from the recent messages.\n",
      " |      \n",
      " |      This function undoes the effects of the `updateMemory` method by taking\n",
      " |      the most recent messages from the stage memory and adding them back \n",
      " |      into the main memory. It is designed to help maintain a consistent \n",
      " |      memory state throughout the agent's execution.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This function may be removed in the near future.\n",
      " |      \n",
      " |      :return: None\n",
      " |      :rtype: None\n",
      " |      \n",
      " |      :example:\n",
      " |          >>> agent.resetMemory()\n",
      " |  \n",
      " |  save_config(self)\n",
      " |      Saves the agent configuration settings to a JSON file.\n",
      " |      \n",
      " |      :param config: A dictionary containing the configuration settings to be saved.\n",
      " |      :type config: dict\n",
      " |      \n",
      " |      :raises FileNotFoundError: If the directory for the configuration file is not found.\n",
      " |      :raises TypeError: If the configuration dictionary contains non-serializable values.\n",
      " |  \n",
      " |  save_state(self)\n",
      " |      Saves the agent state to a file named '.agent-state.pkl' in the output directory.\n",
      " |      This method is registered with atexit to ensure it is called when the program exits.\n",
      " |  \n",
      " |  set_llm(self, llm)\n",
      " |      Set the LLM and handle any related logic.\n",
      " |  \n",
      " |  to_langchain(self)\n",
      " |      This function constructs an object that may be used as an llm in langchain.\n",
      " |      \n",
      " |      :return: a LangChain compatible LLM instance\n",
      " |      :rtype: `BradLLM`\n",
      " |  \n",
      " |  updateMemory(self)\n",
      " |      Thils function lets BRAD reset his memory to focus on specific previous interactions. This is useful\n",
      " |      when BRAD is executing a pipeline and want to manage how input flows from one section to the next.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This function may be removed in the near future.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  llm\n",
      " |      Get the current LLM.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(agent.Agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaefe91-2791-4f9a-95ee-7ae6f169fc54",
   "metadata": {},
   "source": [
    "## 3. LLM Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81dd2db-80dc-47f5-a84b-4000d3d16677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BRAD import llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1140e0-ff1d-4464-bef7-26c67bb67398",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llms.load_openai(\n",
    "    model_name='gpt-3.5-turbo-0125',\n",
    "    api_key='XXXXX'\n",
    "    temperature='1.0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "980bdbb5-461a-447d-a6f7-ff9ae30ca556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_openai in module BRAD.llms:\n",
      "\n",
      "load_openai(model_name='gpt-3.5-turbo-0125', api_key=None, temperature=0)\n",
      "    Loads the OPENAI language model with the specified model name and API key.\n",
      "    \n",
      "    :param model_name: Name of the OPENAI model to load.\n",
      "    :type model_name: str, optional\n",
      "    :param api_key: API key for accessing OPENAI's services. If not provided, it will be prompted.\n",
      "    :type api_key: str, optional\n",
      "    :param temperature: temperature (i.e. creativity or randomness) of the llm\n",
      "    :type temperature: float, optional (default 0)\n",
      "    \n",
      "    :raises AssertionError: If the provided OPENAI API key is not valid.\n",
      "    \n",
      "    :return: The loaded OPENAI language model.\n",
      "    :rtype: langchain_openai.ChatOpenAI\n",
      "    \n",
      "    :example:\n",
      "        >>> openai_model = load_openai()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(llms.load_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60a53910-5754-4668-ae39-47ff6d971158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your NVIDIA API key:  ········\n"
     ]
    }
   ],
   "source": [
    "model = llms.load_nvidia(\n",
    "    model_name='meta/llama3-70b-instruct',\n",
    "#    api_key='XXXXX'\n",
    "    temperature='1.0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5827eea-6f86-4003-b89d-dee111aab9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_nvidia in module BRAD.llms:\n",
      "\n",
      "load_nvidia(model_name='meta/llama3-70b-instruct', nvidia_api_key=None, temperature=None)\n",
      "    Loads the NVIDIA language model with the specified model name and API key.\n",
      "    \n",
      "    :param model_name: Name of the NVIDIA model to load.\n",
      "    :type model_name: str, optional\n",
      "    :param nvidia_api_key: API key for accessing NVIDIA's services. If not provided, it will be prompted.\n",
      "    :type nvidia_api_key: str, optional\n",
      "    :param temperature: temperature (i.e. creativity or randomness) of the llm\n",
      "    :type temperature: float, optional\n",
      "    \n",
      "    :raises AssertionError: If the provided NVIDIA API key is not valid.\n",
      "    \n",
      "    :return: The loaded NVIDIA language model.\n",
      "    :rtype: langchain_nvidia_ai_endpoints.ChatNVIDIA\n",
      "    \n",
      "    :example:\n",
      "        >>> nvidia_model = load_nvidia()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(llms.load_nvidia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3fcd138-94ad-44eb-b829-9711d29f3937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Would you like to use a database with BRAD [Y/N]?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 15:57:25 INFO semantic_router.utils.logger local\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to RAG! The chat log from this conversation will be saved to /home/jpic/C:\\Users\\jpic\\Documents\\BRAD/November 20, 2024 at 03:57:21 PM/log.json. How can I help?\n"
     ]
    }
   ],
   "source": [
    "bot = agent.Agent(llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da8bc6d2-a8b1-4dc2-b2c0-9e6bf69c252f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "BRAD >> 1: \n",
      "Nice to meet you! I am BRAD, which stands for Bioinformatic Retrieval Augmented Data. I'm a chatbot specifically designed to assist with queries related to biology, bioinformatics, genetics, and data science. My capabilities are enhanced by my connection to a vast text database, which enables me to provide answers that are informed by the latest research and findings in these fields.\n",
      "\n",
      "I can operate in various modes to provide accurate and up-to-date information. My Retrieval Augmented Generation module allows me to generate human-like responses based on the patterns and relationships I've learned from the literature. I can also search the web for new articles, ensuring that my knowledge stays current and relevant.\n",
      "\n",
      "In addition, I have access to specialized bioinformatics databases like Gene Ontology and Enrichr, which provides me with a wealth of information on gene functions, pathways, and more. If needed, I can even run Snakemake and MATLAB pipelines to analyze data and generate custom results.\n",
      "\n",
      "Lastly, I can analyze my own code and reflect on my internal processes to improve my performance and provide better responses over time. So, feel free to ask me any questions you have, and I'll do my best to provide helpful and accurate answers!\n",
      "WRITE LOG\n"
     ]
    }
   ],
   "source": [
    "response = bot.invoke(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3791b68f-3379-4307-ae1b-c99dede0d27c",
   "metadata": {},
   "source": [
    "## 4. Integration to LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a453bc3-044e-42ce-a91b-e578c2925156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
